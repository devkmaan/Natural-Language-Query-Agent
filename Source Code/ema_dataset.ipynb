{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjMZYOmEm8Dy"
   },
   "source": [
    "**Author: [Dev Kumar Maan](https://www.linkedin.com/in/dev-kumar-maan-3a6369180/)**\n",
    "\n",
    "**Institution: National Institute of Technology, Delhi**\n",
    "\n",
    "**Email: dev.maan02@gmail.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBOxIJ_veiyP"
   },
   "source": [
    "# Natural Language Query Agent (Dataset Preparation)\n",
    "\n",
    "The primary objective of this project is to develop a Natural Language Query Agent that leverages Large Language Models (LLMs) to provide concise responses to straightforward queries within a substantial dataset comprising lecture notes. \n",
    "\n",
    "This notebook offers a comprehensive guide to preparing the dataset for use in our final pipeline, facilitating answers to conversational questions.\n",
    "\n",
    "> The data sources utilized for this project encompass the following:\n",
    "\n",
    "- [Stanford LLMs Lecture Notes](https://stanford-cs324.github.io/winter2022/lectures/)\n",
    "\n",
    "- [Awesome LLM Milestone Papers](https://github.com/Hannibal046/Awesome-LLM#milestone-papers)\n",
    "\n",
    "- [An Extensive Paper List (and Various Resources) on NLP for Social Good](https://github.com/zhijing-jin/NLP4SocialGood_Papers?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuGG9CbahVfQ"
   },
   "source": [
    "Let's begin by installing the essential libraries and frameworks required to run this project. The following tools are necessary for its proper functionality.\n",
    "\n",
    "- [transformers](https://pypi.org/project/transformers/)\n",
    "- [accelerate](https://pypi.org/project/accelerate/)\n",
    "- [einops](https://pypi.org/project/einops/)\n",
    "- [langchain](https://pypi.org/project/langchain/)\n",
    "- [xformers](https://pypi.org/project/xformers/)\n",
    "- [bitsandbytes](https://pypi.org/project/bitsandbytes/)\n",
    "- [faiss-gpu](https://pypi.org/project/faiss-gpu/)\n",
    "- [sentence_transformers](https://pypi.org/project/sentence-transformers/)\n",
    "- [pypdf](https://pypi.org/project/pypdf/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8b1iCjn-Eim",
    "outputId": "fef879a4-155b-4b67-8c0c-e9be677a138f"
   },
   "outputs": [],
   "source": [
    "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-cpu sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U21Ll4SDBg_d"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rn0R6wAhA8oe",
    "outputId": "85e8660f-c221-4d8e-c275-b7c10fd2e169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\devma\\anaconda3\\lib\\site-packages (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9VI_xKLd-H6l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY2rhOqpiKFo"
   },
   "source": [
    "#### Now, we'll generate the dataset that will be employed in our project. We will utilize the document loaders provided by Langchain, which include:\n",
    "\n",
    "- [WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base): WebBaseLoader represents a robust framework designed for extracting text content from HTML web pages and transforming it into a document format suitable for a wide range of downstream tasks.\n",
    "- [PyPDF](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf): PyPDF is a potent framework crafted for the extraction of text from PDF files.\n",
    "\n",
    "\n",
    "We will retrieve links to webpages and PDF files from the following .txt documents:\n",
    "\n",
    "\tema_dataset_web_links.txt\n",
    "\tema_dataset_pdf_links.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "A1AWYHn1IVrp"
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.rstrip('\\n')\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QmsH-wloJimx"
   },
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    documents_pdf =[]\n",
    "    pdf_links = read_file(file_path)\n",
    "    for link in pdf_links:\n",
    "        loader = PyPDFLoader(link)\n",
    "        pdf = loader.load()\n",
    "        documents_pdf = documents_pdf + pdf\n",
    "    return documents_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "J-sXi_RwIPHQ"
   },
   "outputs": [],
   "source": [
    "file_path = 'ema_dataset_web_links.txt'\n",
    "web_links = read_file(file_path)\n",
    "loader = WebBaseLoader(web_links)\n",
    "documents_web_links = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "mdySANz7Byq2"
   },
   "outputs": [],
   "source": [
    "file_path = 'ema_dataset_pdf_links.txt'\n",
    "documents_pdf = read_pdf(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCKDKU-elJ1p"
   },
   "source": [
    "#### At the end of the process, we'll preserve the custom dataset as a .pkl file, an integral part of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "aIPkx10e-YFV"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits_p0 = text_splitter.split_documents(documents_web_links)\n",
    "all_splits_p1 = text_splitter.split_documents(documents_pdf)\n",
    "\n",
    "all_splits = all_splits_p0 + all_splits_p1\n",
    "\n",
    "with open('ema_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(all_splits, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
